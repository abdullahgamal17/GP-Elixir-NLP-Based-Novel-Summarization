# !pip install pyvis
# !pip install python-louvain

import locale
locale.getpreferredencoding = lambda: "UTF-8"
import pyvis
import networkx as nx
import matplotlib.pyplot as plt
from community import community_louvain
from pyvis.network import Network
import json
import pandas as pd
import csv
import numpy as np
import os
import shutil

PROCESSED_FOLDER = r"files_folder\files_processed"
window_size = 5

def remove_rare_char(book_data):
  """
  Clears character names that appear below a certain threshold, to avoid unnecessary data generated by page numbering, book index, etc.
  """
  for i in range(0,len(book_data['characters'])):
    c = 0
    for x in range(0, len(book_data["characters"][i]["mentions"]["proper"])):
      if (book_data["characters"][i]["mentions"]["proper"][x-c]['c'] < 5):
        book_data["characters"][i]["mentions"]["proper"].pop(x-c)
        c += 1
  return book_data

def get_sentence_id(tokens , entities):
  my_df = entities[['COREF', 'start_token', 'text']]
  sentence_id = []
  for row in my_df.index:
    sentence_id.append(tokens[tokens['token_ID_within_document'] == my_df['start_token'][row]]['sentence_ID'].iloc[0])
  my_df['sentence_id'] = sentence_id
  return my_df
  

def get_char_name(book_data , my_df):
  my_df['char_name'] = 'unknown'
  char_name = []
  for row in my_df.index:
    for char in book_data['characters']:
        if (char['id'] == my_df['COREF'][row] and len(char['mentions']['proper']) > 0  ):
          my_df['char_name'][row] = char['mentions']['proper'][0]['n']
  my_df = my_df[my_df['char_name'] != 'unknown']
  return my_df



def get_names_in_window(my_df , window_size):
  my_df['relations in window'] = 'NAN'
  # reset the index of the DataFrame to start from 0
  my_df = my_df.reset_index(drop=True)

  for row in my_df.index:
    number_list = list(range(my_df['sentence_id'][row], my_df['sentence_id'][row] + window_size))
    temp_list = [my_df['char_name'][row]]
    for i in range(row+1 , row + window_size):
      if (i == len(my_df)):
        break
      if (my_df['sentence_id'][i] in number_list):
        temp_list.append(my_df['char_name'][i])
    my_df['relations in window'][row] = temp_list
  return my_df





def remove_duplicates(lists):
    return [list(set(lst)) for lst in lists]


def generate_relationship_df(char_unique):
  relationships = []
  for two_sentence in char_unique:
    if len(two_sentence) > 1:
      for idx, a in enumerate(two_sentence[:-1]):
        b = two_sentence[idx + 1]
        relationships.append({"source": a, "target": b})
          
  relationship_df = pd.DataFrame(relationships)
  # Sort the cases with a->b and b->a
  relationship_df = pd.DataFrame(np.sort(relationship_df.values, axis = 1), columns = relationship_df.columns)
  relationship_df["value"] = 1
  relationship_df = relationship_df.groupby(["source","target"], sort=False,  as_index=False).sum()
  return relationship_df



def ploting(target_folder,relationship_df):
  #create networkx graph
  nx_graph = nx.from_pandas_edgelist(relationship_df,
                                    source = "source",
                                    target = "target",
                                    edge_attr = "value"
                                    ) 
  #use kawai layout
  plt.figure(figsize = (10,10))
  layout = nx.kamada_kawai_layout(nx_graph)
  communities =community_louvain.best_partition(nx_graph)
  nx.set_node_attributes(nx_graph , communities, 'group')

  # #change nodesize depending on degeree of node (how many edges)
  node_degree = dict(nx_graph.degree)
  network = Network(notebook = True, width = "1000px", height = "600px",  bgcolor="rgba(255, 99, 71, 0)" , font_color="white" )
  nx.set_node_attributes(nx_graph, node_degree, "size")
  network.from_nx(nx_graph)
  network.show("Character Network.html")
  shutil.move("Character Network.html",target_folder)




def main(book_data_path, entities_path, tokens_path, window_size):
  with open (book_data_path, "r") as f:
    book_data = json.load(f)

  book_data = remove_rare_char(book_data)
  entities = pd.read_csv(entities_path, delimiter = '\t')
  entities = entities[entities['cat']  == 'PER']
  tokens = pd.read_csv(tokens_path, delimiter = '\t' ,  on_bad_lines='skip', quoting=csv.QUOTE_NONE)

  path_seperated = book_data_path.split("\\")
  path = path_seperated[:-2]
  path.append("needed_files")
  target_folder = os.path.join(*path)

  my_df = get_sentence_id(tokens, entities)
  my_df = get_char_name(book_data, my_df)

  my_df  =  get_names_in_window(my_df, window_size)
  my_list = list(my_df['relations in window'])

    # Remove duplicated characters that are next to each other
  char_unique = remove_duplicates(my_list)
  relationship_df = generate_relationship_df(char_unique)
  relationship_df = relationship_df[relationship_df['value'] >= 14]
  ploting(target_folder,relationship_df)


def get_needed_pathes(req_id):
  book_data_path = None
  entities_path = None
  tokens_path = None
  for folder in os.listdir(PROCESSED_FOLDER):
    if folder.startswith(str(req_id)):
      directory = os.path.join(PROCESSED_FOLDER, folder,"needed_files")
      for file in os.listdir(directory):
        if file.endswith(".book"):
          book_data_path = os.path.join(directory,file)
        elif file.endswith(".entities"):
          entities_path = os.path.join(directory,file)
        elif file.endswith(".tokens"):
          tokens_path = os.path.join(directory,file)
  return book_data_path, entities_path, tokens_path

def run_char_network(req_id):
  book_data_path, entities_path, tokens_path = get_needed_pathes(req_id)
  main(book_data_path, entities_path, tokens_path, window_size)



